import streamlit as st
import pandas as pd
import plotly.express as px
import random
import streamlit_shadcn_ui as ui
from wordcloud import WordCloud 
import matplotlib.pyplot as plt

# Apply Custom CSS
st.markdown(
    """
    <style>
    body {
        background-color: #FAF3E0;
        color: #5C3D2E;
        font-family: 'Arial', sans-serif;
    }
    .stTitle {
        color: #D2691E;
        text-align: center;
        font-weight: bold;
    }
    .stSidebar {
        padding: 5px;
        border-radius: 5px;
    }
    .stDataFrame {
        background-color: #FFEBCD;
        border-radius: 10px;
        padding: 5px;
    }
    .block-container {
        padding: 1rem;
    }
    </style>
    """,
    unsafe_allow_html=True
)

# Load the data from the CSV file
# @st.cache_data  # Cache the data to improve performance
def load_data():
    file_path = "/home/rei/Documents/Project/AI/child_report.csv"
    df = pd.read_csv(file_path)
    df['time'] = pd.to_datetime(df['time'])  # Convert 'time' column to datetime
    return df

df = load_data()

def generate_random_topics():
    topics = ["è¯†å­—", "æ•°å­¦", "è®¤è¯†ä¸–ç•Œ", "è‰ºæœ¯ä¸è®¾è®¡"]
    topic_proportions = {topic: random.randint(10, 40) for topic in topics}  # Random proportions
    return topic_proportions

def generate_word_frequencies(topic):
    words = {
        "è¯†å­—":["reading", "writing", "books", "story", "alphabet"],
        "æ•°å­¦": ["numbers", "counting", "addition", "subtraction", "shapes"],
        "è®¤è¯†ä¸–ç•Œ": ["animals", "plants", "weather", "seasons", "space"],
        "è‰ºæœ¯ä¸è®¾è®¡": ["painting", "sketching", "music", "dancing", "colors"],
    }
    return {word: random.randint(5, 20) for word in words[topic]}  # Random word frequencies


# Cumulative unique words (mock data for demonstration)
df['Unique Words'] = [len(set(sentence.split())) for sentence in df['sentence']]
df['Cumulative Unique Words'] = df['Unique Words'].cumsum()

# Dashboard Title
st.title("EchooGrow")
st.warning("æ¬¢è¿ä½¿ç”¨ EchooGrow è¯­è¨€å‘å±•ä»ªè¡¨ç›˜ï¼è¿½è¸ªæ‚¨å­©å­çš„è¯­è¨€å‘å±•æƒ…å†µ")

# Interactive Dashboard Summary
st.subheader("ğŸ£ æœ¬æœˆæ€»ç»“")

child_name = "çŒ«çŒ«"
total_words = df['sentence'].apply(lambda x: len(x.split())).sum()
avg_sentence_length = round(df['sentence'].apply(lambda x: len(x.split())).mean(), 2)
predominant_sentiment = df['emotion'].mode()[0]
topic_proportions = generate_random_topics()
top_topics = sorted(topic_proportions, key=topic_proportions.get, reverse=True)[:2]

summary = (
    f"æœ¬æœˆï¼Œ<span style='color:#EF8985; font-weight:bold;'>{child_name}</span> æ€»å…±è¯´äº† "
    f"<span style='color:#EF8985; font-weight:bold;'>{total_words}</span> ä¸ªå•è¯ã€‚ "
    f"ä»–ä»¬çš„å¹³å‡å¥å­é•¿åº¦ä¸º <span style='color:#EF8985; font-weight:bold;'>{avg_sentence_length}</span> ä¸ªå•è¯ï¼Œ "
    f"æ¯”ä¸Šä¸ªæœˆæœ‰æ‰€å¢åŠ ï¼Œè¡¨æ˜è¯­è¨€å¤æ‚æ€§åœ¨æé«˜ã€‚ "
    f"ä»–ä»¬ä¸»è¦è¡¨è¾¾äº† <span style='color:#EF8985; font-weight:bold;'>{predominant_sentiment}</span> çš„æƒ…ç»ªï¼Œ "
    f"å¶å°”åœ¨è®¨è®º <span style='color:#EF8985; font-weight:bold;'>{top_topics[0]}</span> æ—¶è¡¨ç°å‡º "
    f"<span style='color:#EF8985; font-weight:bold;'>{random.choice(df['emotion'].unique())}</span> çš„æƒ…ç»ªã€‚ "
    f"è®¨è®ºæœ€å¤šçš„ä¸»é¢˜æ˜¯ <span style='color:#EF8985; font-weight:bold;'>{top_topics[0]}</span> å’Œ "
    f"<span style='color:#EF8985; font-weight:bold;'>{top_topics[1]}</span>ã€‚"
)
st.markdown(
    f"<div style= padding:10px; border-radius:10px;'>"
    f"{summary}</div>",
    unsafe_allow_html=True
)

# Language Proficiency Visualization
st.subheader("ğŸ—£ï¸ è¯­è¨€èƒ½åŠ›å‘å±•è¶‹åŠ¿")
ui.card()
fig_mlu = px.line(df, x='time', y=df['sentence'].apply(lambda x: len(x.split())), title="å¹³å‡å¥å­é•¿åº¦éšæ—¶é—´å˜åŒ–", markers=True, color_discrete_sequence=["#5cd6c4"])
st.plotly_chart(fig_mlu, use_container_width=True)

ui.card()
fig_words = px.line(df, x='time', y=df['sentence'].apply(lambda x: len(x.split())).cumsum(), title="æ€»å•è¯æ•°éšæ—¶é—´å˜åŒ–", markers=True, color_discrete_sequence=["#EF8985"])
st.plotly_chart(fig_words, use_container_width=True)

ui.card()
fig_vocab = px.line(df, x='time', y='Cumulative Unique Words', title="ç´¯è®¡å”¯ä¸€å•è¯æ•°éšæ—¶é—´å˜åŒ–", markers=True, color_discrete_sequence=["#eec843"])
st.plotly_chart(fig_vocab, use_container_width=True)

# Emotional Trend Visualization
st.subheader("ğŸ’– æƒ…ç»ªè¶‹åŠ¿")
ui.card()
fig_emotion = px.histogram(df, x='emotion', title="æƒ…ç»ªåˆ†å¸ƒéšæ—¶é—´å˜åŒ–", color='emotion', color_discrete_map={"Joy": "#5cd6c4", "Sadness": "#e6d5f4", "Anger": "#EF8985", "Neutral": "#eec843", "Fear": "#a8d5e2", "Disgust": "#d4a5a5", "Excitement": "#ffcc99"})
st.plotly_chart(fig_emotion, use_container_width=True)

ui.card()
fig_sentiment_trend = px.line(df, x='time', y='sentiment_score', title="æƒ…ç»ªå¾—åˆ†éšæ—¶é—´å˜åŒ–", markers=True, line_shape='spline', color_discrete_sequence=["#5cd6c4"])
fig_sentiment_trend.update_traces(line=dict(width=3))
st.plotly_chart(fig_sentiment_trend, use_container_width=True)


# Interests Tendencies Visualization
st.subheader("ğŸ¨ å…´è¶£è¶‹åŠ¿")

topic_df = pd.DataFrame(list(topic_proportions.items()), columns=["Topic", "Proportion"])

# Pie chart for topic proportions
st.write("### è®¨è®ºä¸»é¢˜æ¯”ä¾‹")
fig_topics = px.pie(topic_df, names="Topic", values="Proportion", title="è®¨è®ºä¸»é¢˜æ¯”ä¾‹", color_discrete_sequence=px.colors.qualitative.Pastel)
st.plotly_chart(fig_topics, use_container_width=True)


# Interactive Word Cloud
st.write("### äº’åŠ¨ä¸»é¢˜è¯äº‘")
selected_topic = st.selectbox("é€‰æ‹©ä¸€ä¸ªä¸»é¢˜è¿›è¡Œæ¢ç´¢", topic_df["Topic"].tolist())

# Generate word frequencies for the selected topic
word_frequencies = generate_word_frequencies(selected_topic)

# Create and display the word cloud
st.write(f"**Most Common Words in '{selected_topic}':**")
wordcloud = WordCloud(width=400, height=200, background_color="white").generate_from_frequencies(word_frequencies)
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
st.pyplot(plt)

# Display word frequencies as a table
st.write("**Word Frequencies:**")
word_freq_df = pd.DataFrame(list(word_frequencies.items()), columns=["Word", "Frequency"])
st.dataframe(word_freq_df)


st.write("ğŸ“Š Filtered Data:", df)